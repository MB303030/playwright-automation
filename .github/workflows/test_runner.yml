name: Test Runner

on:
  workflow_call:
    inputs:
      test_type:
        required: true
        type: string
      test_suite:
        required: true
        type: string
      browser:
        required: true
        type: string
      environment:
        required: true
        type: string
      custom_tags:
        required: false
        type: string
        default: ''
      priority:
        required: true
        type: string
      test_path:
        required: true
        type: string
      run_id:
        required: true
        type: string

env:
  CI: true

jobs:
  run_tests:
    runs-on: ubuntu-latest
    continue-on-error: true  # ğŸ‘ˆ Continue even if tests fail
    
    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: âš™ï¸ Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'
          cache: 'npm'
      
      - name: ğŸ“¦ Install dependencies
        run: |
          echo "ğŸ“¦ Installing dependencies..."
          if [ -f "package.json" ]; then
            npm ci || echo "âš ï¸ npm ci failed, trying npm install..."
            npm install || echo "âš ï¸ npm install also failed"
          else
            echo "â„¹ï¸ No package.json found, skipping npm install"
          fi
      
      - name: ğŸƒ Execute Tests
        id: execute
        continue-on-error: true  # ğŸ‘ˆ Continue even if tests fail
        run: |
          echo "ğŸš€ Running tests for: ${{ inputs.test_type }}"
          echo "ğŸ“‚ Test path: ${{ inputs.test_path }}"
          echo "ğŸŒ Browser: ${{ inputs.browser }}"
          echo "ğŸ¢ Environment: ${{ inputs.environment }}"
          echo "ğŸ¯ Priority: ${{ inputs.priority }}"
          echo "ğŸ†” Run ID: ${{ inputs.run_id }}"
          
          START_TIME=$(date +%s)
          
          # Create test results directory
          mkdir -p ./test-results/${{ inputs.run_id }}
          
          # Check if test directory exists
          if [ -d "${{ inputs.test_path }}" ] || [ -f "${{ inputs.test_path }}" ]; then
            echo "âœ… Test path exists: ${{ inputs.test_path }}"
            
            # For now, simulate test execution
            echo "ğŸ“ Running simulated tests..."
            sleep 3
            
            # Create mock test results
            echo '{
              "total_tests": 42,
              "passed": 38,
              "failed": 3,
              "skipped": 1,
              "duration": 127,
              "success_rate": 90,
              "test_cases": [
                {"name": "Login Test", "status": "passed", "duration": 2.5},
                {"name": "Dashboard Load", "status": "passed", "duration": 1.8},
                {"name": "API Validation", "status": "failed", "duration": 4.2, "error": "Timeout exceeded"}
              ]
            }' > ./test-results/${{ inputs.run_id }}/results.json
            
            echo "âœ… Tests completed successfully!"
            EXIT_CODE=0
          else
            echo "âš ï¸ Test path does not exist: ${{ inputs.test_path }}"
            echo "Creating dummy test results..."
            
            # Create dummy results for non-existent test path
            echo '{
              "total_tests": 0,
              "passed": 0,
              "failed": 1,
              "skipped": 0,
              "duration": 0,
              "success_rate": 0,
              "error": "Test path does not exist: '"${{ inputs.test_path }}"'",
              "test_cases": []
            }' > ./test-results/${{ inputs.run_id }}/results.json
            
            EXIT_CODE=1
          fi
          
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          
          echo "â±ï¸ Execution time: $DURATION seconds"
          echo "exit_code=$EXIT_CODE" >> $GITHUB_OUTPUT
          echo "duration=$DURATION" >> $GITHUB_OUTPUT
      
      - name: ğŸ“Š Create Statistics
        id: stats
        run: |
          echo "ğŸ“Š Creating test statistics..."
          
          RESULTS_FILE="./test-results/${{ inputs.run_id }}/results.json"
          
          if [ -f "$RESULTS_FILE" ]; then
            # Parse JSON with grep (no jq dependency)
            TOTAL_TESTS=$(grep -o '"total_tests"[[:space:]]*:[[:space:]]*[0-9]*' "$RESULTS_FILE" | head -1 | grep -o '[0-9]*' || echo "0")
            PASSED_TESTS=$(grep -o '"passed"[[:space:]]*:[[:space:]]*[0-9]*' "$RESULTS_FILE" | head -1 | grep -o '[0-9]*' || echo "0")
            FAILED_TESTS=$(grep -o '"failed"[[:space:]]*:[[:space:]]*[0-9]*' "$RESULTS_FILE" | head -1 | grep -o '[0-9]*' || echo "0")
            SKIPPED_TESTS=$(grep -o '"skipped"[[:space:]]*:[[:space:]]*[0-9]*' "$RESULTS_FILE" | head -1 | grep -o '[0-9]*' || echo "0")
            DURATION=$(grep -o '"duration"[[:space:]]*:[[:space:]]*[0-9]*' "$RESULTS_FILE" | head -1 | grep -o '[0-9]*' || echo "0")
          else
            echo "âš ï¸ No results file found, creating default"
            TOTAL_TESTS=0
            PASSED_TESTS=0
            FAILED_TESTS=1
            SKIPPED_TESTS=0
            DURATION=0
          fi
          
          # Calculate success rate
          if [ "$TOTAL_TESTS" -gt 0 ]; then
            SUCCESS_RATE=$((PASSED_TESTS * 100 / TOTAL_TESTS))
          else
            SUCCESS_RATE=0
          fi
          
          # Save statistics to file
          echo "{
            \"total_tests\": $TOTAL_TESTS,
            \"passed_tests\": $PASSED_TESTS,
            \"failed_tests\": $FAILED_TESTS,
            \"skipped_tests\": $SKIPPED_TESTS,
            \"duration\": $DURATION,
            \"success_rate\": $SUCCESS_RATE,
            \"test_type\": \"${{ inputs.test_type }}\",
            \"test_suite\": \"${{ inputs.test_suite }}\",
            \"browser\": \"${{ inputs.browser }}\",
            \"environment\": \"${{ inputs.environment }}\",
            \"priority\": \"${{ inputs.priority }}\",
            \"run_id\": \"${{ inputs.run_id }}\"
          }" > ./test-results/${{ inputs.run_id }}/statistics.json
          
          echo "âœ… Statistics created!"
      
      - name: ğŸ“¦ Upload Test Results
        if: always()  # ğŸ‘ˆ ALWAYS upload results, even if tests fail
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ inputs.run_id }}
          path: ./test-results/${{ inputs.run_id }}
          retention-days: 7
